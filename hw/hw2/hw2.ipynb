{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "def calculate_fire_type_percentage(filename):\n",
    "    total_fire_pokemons = 0\n",
    "    fire_pokemons_above_level_40 = 0\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            if row['type'] == 'fire':\n",
    "                total_fire_pokemons += 1\n",
    "                if float(row['level']) >= 40:\n",
    "                    fire_pokemons_above_level_40 += 1\n",
    "    \n",
    "    percentage = (fire_pokemons_above_level_40 / total_fire_pokemons) * 100 if total_fire_pokemons > 0 else 0\n",
    "    return round(percentage)\n",
    "\n",
    "percentage = calculate_fire_type_percentage('../data/pokemonTrain.csv')\n",
    "\n",
    "with open('pokemon1.txt', 'w') as output_file:\n",
    "    output_file.write(f\"Percentage of fire type Pokemons at or above level 40 = {percentage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n",
    "[13 pts] Fill in the missing (NaN) values in the Attack (\"atk\"), Defense (\"def\") and Hit Points (\"hp\") columns as follows:\n",
    "- Set the pokemon level threshold to 40.\n",
    "- For a Pokemon having level above the threshold (i.e. > 40), fill in the missing value for atk/def/hp with the average values of atk/def/hp of Pokemons with level > 40. So, for instance, you would substitute the missing \"atk\" value for Magmar (level 44), with the average \"atk\" value for Pokemons with level > 40. Round the average to one decimal place.\n",
    "- For a Pokemon having level equal to or below the threshold (i.e. <= 40), fill in the missing value for atk/def/hp with the average values of atk/def/hp of Pokemons with level <= 40. Round the average to one decimal place.\n",
    "\n",
    "**After performing #2 and #3, write the modified data to another csv file named \"pokemonResult.csv\". This result file should have all of the rows from the input file - rows that were modified as well as rows that were not modified. If you do not write the modified data to another CSV file, or your output file name is not exactly as required, you will get 0 points.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "def get_most_common_type(weakness):\n",
    "    types = []\n",
    "    for w in weakness.split(','):\n",
    "        types.append(w.strip())\n",
    "    type_counts = Counter(types)\n",
    "    most_common_type = min(type_counts, key=lambda k: (-type_counts[k], k))\n",
    "    return most_common_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "def fill_missing_types(filename):\n",
    "    filled_data = []\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            if row['type'] == 'NaN':\n",
    "                most_common_type = get_most_common_type(row['weakness'])\n",
    "                row['type'] = most_common_type\n",
    "            filled_data.append(row)\n",
    "    return filled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(data):\n",
    "    atk_sum_over_40 = 0\n",
    "    def_sum_over_40 = 0\n",
    "    hp_sum_over_40 = 0\n",
    "    atk_count_over_40 = 0\n",
    "    def_count_over_40 = 0\n",
    "    hp_count_over_40 = 0\n",
    "    atk_sum_below_40 = 0\n",
    "    def_sum_below_40 = 0\n",
    "    hp_sum_below_40 = 0\n",
    "    atk_count_below_40 = 0\n",
    "    def_count_below_40 = 0\n",
    "    hp_count_below_40 = 0\n",
    "    \n",
    "    for row in data:\n",
    "        if row['atk'] != 'NaN':\n",
    "            if float(row['level']) > 40:\n",
    "                atk_sum_over_40 += float(row['atk'])\n",
    "                atk_count_over_40 += 1\n",
    "            else:\n",
    "                atk_sum_below_40 += float(row['atk'])\n",
    "                atk_count_below_40 += 1\n",
    "        \n",
    "        if row['def'] != 'NaN':\n",
    "            if float(row['level']) > 40:\n",
    "                def_sum_over_40 += float(row['def'])\n",
    "                def_count_over_40 += 1\n",
    "            else:\n",
    "                def_sum_below_40 += float(row['def'])\n",
    "                def_count_below_40 += 1\n",
    "        \n",
    "        if row['hp'] != 'NaN':\n",
    "            if float(row['level']) > 40:\n",
    "                hp_sum_over_40 += float(row['hp'])\n",
    "                hp_count_over_40 += 1\n",
    "            else:\n",
    "                hp_sum_below_40 += float(row['hp'])\n",
    "                hp_count_below_40 += 1\n",
    "    \n",
    "    avg_atk_over_40 = round(atk_sum_over_40 / atk_count_over_40, 1) if atk_count_over_40 > 0 else 0\n",
    "    avg_def_over_40 = round(def_sum_over_40 / def_count_over_40, 1) if def_count_over_40 > 0 else 0\n",
    "    avg_hp_over_40 = round(hp_sum_over_40 / hp_count_over_40, 1) if hp_count_over_40 > 0 else 0\n",
    "    avg_atk_below_40 = round(atk_sum_below_40 / atk_count_below_40, 1) if atk_count_below_40 > 0 else 0\n",
    "    avg_def_below_40 = round(def_sum_below_40 / def_count_below_40, 1) if def_count_below_40 > 0 else 0\n",
    "    avg_hp_below_40 = round(hp_sum_below_40 / hp_count_below_40, 1) if hp_count_below_40 > 0 else 0\n",
    "    \n",
    "    for row in data:\n",
    "        if row['atk'] == 'NaN':\n",
    "            if float(row['level']) > 40:\n",
    "                row['atk'] = str(avg_atk_over_40)\n",
    "            else:\n",
    "                row['atk'] = str(avg_atk_below_40)\n",
    "        \n",
    "        if row['def'] == 'NaN':\n",
    "            if float(row['level']) > 40:\n",
    "                row['def'] = str(avg_def_over_40)\n",
    "            else:\n",
    "                row['def'] = str(avg_def_below_40)\n",
    "        \n",
    "        if row['hp'] == 'NaN':\n",
    "            if float(row['level']) > 40:\n",
    "                row['hp'] = str(avg_hp_over_40)\n",
    "            else:\n",
    "                row['hp'] = str(avg_hp_below_40)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(data, filename):\n",
    "    with open(filename, 'w', newline='') as output_file:\n",
    "        fieldnames = data[0].keys()\n",
    "        writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../data/pokemonTrain.csv'\n",
    "    \n",
    "    # Fill missing types\n",
    "data = fill_missing_types(filename)\n",
    "    \n",
    "    # Fill missing values\n",
    "modified_data = fill_missing_values(data)\n",
    "    \n",
    "    # Write modified data to CSV\n",
    "write_to_csv(modified_data, '../data/pokemonResult.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4\n",
    "[10 pts] Create a dictionary that maps pokemon types to their personalities. This dictionary would map a string to a list of strings. For example:\n",
    "     {\"fire\": [\"docile\", \"modest\", ...], \"normal\": [\"mild\", \"relaxed\", ...],  ...}\n",
    "Your dictionary should have the keys ordered alphabetically, and also items ordered alphabetically in the values list, as shown in the example above.\n",
    "\n",
    "Print the dictionary in the following format:\n",
    "\n",
    "   Pokemon type to personality mapping:\n",
    "\n",
    "      normal: mild, relaxed, ...\n",
    "      fire: docile, modest, ...\n",
    "      ...\n",
    "Print the dictionary to a file named \"pokemon4.txt\"\n",
    "If you do not print to a file, or your output file name is not exactly as required, you will get 0 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug': {'careful'},\n",
       " 'electric': {'hardy'},\n",
       " 'fairy': {'impish', 'naughty', 'sassy'},\n",
       " 'fighting': {'adamant',\n",
       "  'calm',\n",
       "  'careful',\n",
       "  'lax',\n",
       "  'lonely',\n",
       "  'relaxed',\n",
       "  'serious'},\n",
       " 'fire': {'bold',\n",
       "  'brave',\n",
       "  'gentle',\n",
       "  'hardy',\n",
       "  'impish',\n",
       "  'lax',\n",
       "  'mild',\n",
       "  'modest',\n",
       "  'naive',\n",
       "  'naughty',\n",
       "  'quiet',\n",
       "  'rash',\n",
       "  'timid'},\n",
       " 'flying': {'hasty', 'impish', 'serious'},\n",
       " 'grass': {'adamant',\n",
       "  'bashful',\n",
       "  'bold',\n",
       "  'calm',\n",
       "  'docile',\n",
       "  'gentle',\n",
       "  'lonely',\n",
       "  'naive',\n",
       "  'quiet',\n",
       "  'sassy'},\n",
       " 'ground': {'bashful', 'gentle', 'hardy', 'quiet'},\n",
       " 'normal': {'brave', 'jolly', 'mild', 'naughty', 'quiet'},\n",
       " 'rock': {'bashful', 'docile', 'impish', 'mild', 'naughty'},\n",
       " 'water': {'bashful', 'bold', 'docile', 'hardy', 'impish', 'jolly', 'naughty'}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_type_to_personality_mapping(filename):\n",
    "    personality_mapping = {}\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            if row['type'] not in personality_mapping:\n",
    "                personality_mapping[row['type']] = set()\n",
    "            personality_mapping[row['type']].add(row['personality'])\n",
    "    \n",
    "    sorted_mapping = dict(sorted(personality_mapping.items()))\n",
    "    \n",
    "    with open('pokemon4.txt', 'w') as file:\n",
    "        file.write(\"Pokemon type to personality mapping:\\n\\n\")\n",
    "        for pokemon_type, personalities in sorted_mapping.items():\n",
    "            file.write(f\"{pokemon_type}: {', '.join(sorted(personalities))}\\n\")\n",
    "    \n",
    "    return sorted_mapping\n",
    "\n",
    "create_type_to_personality_mapping(filename = '../data/pokemonResult.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5\n",
    "[5 pts] Find out the average Hit Points (\"hp\") for pokemons of stage 3.0.\n",
    "Your program should print the value as follows (replace ... with value):\n",
    "\n",
    "Average hit point for Pokemons of stage 3.0 = ...\n",
    "You should round off the value, like in #1 above.\n",
    "\n",
    "Print the value to a file named \"pokemon5.txt\"\n",
    "If you do not print to a file, or your output file name is not exactly as required, you will get 0 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_average_hit_points(filename):\n",
    "    total_hit_points = 0\n",
    "    count_stage_3 = 0\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            if float(row['stage']) == 3.0:\n",
    "                count_stage_3 += 1\n",
    "                total_hit_points += float(row['hp'])\n",
    "    \n",
    "    average_hp = round(total_hit_points / count_stage_3) if count_stage_3 > 0 else 0\n",
    "    \n",
    "    with open('pokemon5.txt', 'w') as file:\n",
    "        file.write(f\"Average hit point for Pokemons of stage 3.0 = {average_hp}\")\n",
    "    \n",
    "    return average_hp\n",
    "calculate_average_hit_points('../data/pokemonResult.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Covid HW\n",
    "- [5 pts] In the age column, wherever there is a range of values, replace it by the rounded off average value. E.g., for 10-14 substitute 12. (Rounding should be done like in 1.1). You might want to use regular expressions here, but it is not required.\n",
    "- [6 pts] Change the date format for the date columns - date_onset_symptomssss, date_admission_hospital and date_confirmation from dd.mm.yyyy to mm.dd.yyyy. Again, you can use regexps here, but it is not required.\n",
    "- [7 pts] Fill in the missing (NaN) \"latitude\" and \"longitude\" values by the average of the latitude and longitude values for the province where the case was recorded. Round the average to 2 decimal places.\n",
    "- [7 pts] Fill in the missing “city” values by the most occurring city value in that province. In case of a tie, use the city that appears first in alphabetical order.\n",
    "- [10 pts] Fill in the missing \"symptoms\" values by the single most frequent symptoms in the province where the case was recorded. In case of a tie, use the symptoms that appears first in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [5 pts] In the age column, wherever there is a range of values, replace it by the rounded off average value. E.g., for 10-14 substitute 12. (Rounding should be done like in 1.1). You might want to use regular expressions here, but it is not required.\n",
    "\n",
    "def age_col(f):\n",
    "    with open(f, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        header = next(reader)\n",
    "        for row in reader:\n",
    "            if '-' in row['age']:\n",
    "                ages = row['age'].split('-')\n",
    "                avg_age = (int(ages[0]) + int(ages[1])) / 2\n",
    "                row['age'] = str(avg_age)\n",
    "                # print(row['age'])\n",
    "    return\n",
    "\n",
    "age_col('../data/covidTrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [6 pts] Change the date format for the date columns - date_onset_symptomss, date_admission_hospital and date_confirmation from dd.mm.yyyy to mm.dd.yyyy. Again, you can use regexps here, but it is not required.\n",
    "\n",
    "def date_col(f):\n",
    "    with open(f, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        header = next(reader)\n",
    "        for row in reader:\n",
    "            if '.' in row['date_onset_symptomss']:\n",
    "                date = row['date_onset_symptomss'].split('.')\n",
    "                day, month, year = date[0], date[1], date[2]\n",
    "                # print(day, month, year)\n",
    "                row['date_onset_symptomss'] = month + \".\" + day + \".\"+ year\n",
    "                # print(row['date_onset_symptomss'])\n",
    "    return \n",
    "\n",
    "date_col('../data/covidTrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [7 pts] Fill in the missing (NaN) \"latitude\" and \"longitude\" values by the average of the latitude and longitude values for the province where the case was recorded. Round the average to 2 decimal places.\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def replace_empty_vals(f):\n",
    "    province_data = defaultdict(list)\n",
    "\n",
    "    with open(f, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            province_data[row['province']].append(row)\n",
    "\n",
    "    for province, data in province_data.items():\n",
    "        lat_sum = 0\n",
    "        long_sum = 0\n",
    "        count = 0\n",
    "        for entry in data:\n",
    "            if entry['latitude'].lower() != 'nan' and entry['longitude'].lower() != 'nan':\n",
    "                lat_sum += float(entry['latitude'])\n",
    "                long_sum += float(entry['longitude'])\n",
    "                count += 1\n",
    "\n",
    "        if count > 0:\n",
    "            lat_avg = round(lat_sum / count, 2)\n",
    "            long_avg = round(long_sum / count, 2)\n",
    "\n",
    "            for entry in data:\n",
    "                if entry['latitude'].lower() == 'nan' or entry['longitude'].lower() == 'nan':\n",
    "                    entry['latitude'] = str(lat_avg)\n",
    "                    entry['longitude'] = str(long_avg)\n",
    "                    \n",
    "    with open(f, 'w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=reader.fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(row for province_data_list in province_data.values() for row in province_data_list)\n",
    "\n",
    "replace_empty_vals('../data/covidTrain.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m         writer\u001b[39m.\u001b[39mwriterows(row \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m reader)\n\u001b[1;32m     44\u001b[0m \u001b[39m# Call the function with the file path\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m fill_missing_city_values(\u001b[39m'\u001b[39;49m\u001b[39m../data/covidTrain.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[18], line 42\u001b[0m, in \u001b[0;36mfill_missing_city_values\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     40\u001b[0m writer \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mDictWriter(file, fieldnames\u001b[39m=\u001b[39mheader)\n\u001b[1;32m     41\u001b[0m writer\u001b[39m.\u001b[39mwriteheader()\n\u001b[0;32m---> 42\u001b[0m writer\u001b[39m.\u001b[39;49mwriterows(row \u001b[39mfor\u001b[39;49;00m row \u001b[39min\u001b[39;49;00m reader)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/csv.py:157\u001b[0m, in \u001b[0;36mDictWriter.writerows\u001b[0;34m(self, rowdicts)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwriterows\u001b[39m(\u001b[39mself\u001b[39m, rowdicts):\n\u001b[0;32m--> 157\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwriter\u001b[39m.\u001b[39;49mwriterows(\u001b[39mmap\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dict_to_list, rowdicts))\n",
      "Cell \u001b[0;32mIn[18], line 42\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m writer \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mDictWriter(file, fieldnames\u001b[39m=\u001b[39mheader)\n\u001b[1;32m     41\u001b[0m writer\u001b[39m.\u001b[39mwriteheader()\n\u001b[0;32m---> 42\u001b[0m writer\u001b[39m.\u001b[39mwriterows(row \u001b[39mfor\u001b[39;49;00m row \u001b[39min\u001b[39;49;00m reader)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/csv.py:111\u001b[0m, in \u001b[0;36mDictReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_num \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    109\u001b[0m     \u001b[39m# Used only for its side effect.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfieldnames\n\u001b[0;32m--> 111\u001b[0m row \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreader)\n\u001b[1;32m    112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_num \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreader\u001b[39m.\u001b[39mline_num\n\u001b[1;32m    114\u001b[0m \u001b[39m# unlike the basic reader, we prefer not to return blanks,\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39m# because we will typically wind up with a dict full of None\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m# values\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: I/O operation on closed file."
     ]
    }
   ],
   "source": [
    "# [7 pts] Fill in the missing “city” values by the most occurring city value in that province. In case of a tie, use the city that appears first in alphabetical order.\n",
    "from collections import Counter\n",
    "\n",
    "def fill_missing_city_values(f):\n",
    "    # Count occurrences of cities for each province\n",
    "    province_city_counts = {}\n",
    "    \n",
    "    with open(f, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            if row['city'].lower() != 'nan':\n",
    "                province = row['province']\n",
    "                city = row['city']\n",
    "                if province not in province_city_counts:\n",
    "                    province_city_counts[province] = Counter()\n",
    "                province_city_counts[province][city] += 1\n",
    "    \n",
    "    # Replace missing city values\n",
    "    with open(f, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            if row['city'].lower() == 'nan':\n",
    "                province = row['province']\n",
    "                if province in province_city_counts:\n",
    "                    most_common_cities = province_city_counts[province].most_common()\n",
    "                    if most_common_cities:\n",
    "                        # Sort by city name alphabetically in case of a tie\n",
    "                        most_common_cities.sort(key=lambda x: (x[1], x[0]))\n",
    "                        row['city'] = most_common_cities[0][0]\n",
    "                else:\n",
    "                    # If no data for the province, fill with 'Unknown'\n",
    "                    row['city'] = 'Unknown'\n",
    "\n",
    "    # Write the updated data back to the file\n",
    "    with open(f, 'w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(row for row in reader)\n",
    "\n",
    "# Call the function with the file path\n",
    "fill_missing_city_values('../data/covidTrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m         writer\u001b[39m.\u001b[39mwriterows(row \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m reader)\n\u001b[1;32m     41\u001b[0m \u001b[39m# Call the function with the file path\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m fill_missing_symptoms_values(\u001b[39m'\u001b[39;49m\u001b[39m../data/covidTrain.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[22], line 39\u001b[0m, in \u001b[0;36mfill_missing_symptoms_values\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     37\u001b[0m writer \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mDictWriter(file, fieldnames\u001b[39m=\u001b[39mheader)\n\u001b[1;32m     38\u001b[0m writer\u001b[39m.\u001b[39mwriteheader()\n\u001b[0;32m---> 39\u001b[0m writer\u001b[39m.\u001b[39;49mwriterows(row \u001b[39mfor\u001b[39;49;00m row \u001b[39min\u001b[39;49;00m reader)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/csv.py:157\u001b[0m, in \u001b[0;36mDictWriter.writerows\u001b[0;34m(self, rowdicts)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwriterows\u001b[39m(\u001b[39mself\u001b[39m, rowdicts):\n\u001b[0;32m--> 157\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwriter\u001b[39m.\u001b[39;49mwriterows(\u001b[39mmap\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dict_to_list, rowdicts))\n",
      "Cell \u001b[0;32mIn[22], line 39\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     37\u001b[0m writer \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mDictWriter(file, fieldnames\u001b[39m=\u001b[39mheader)\n\u001b[1;32m     38\u001b[0m writer\u001b[39m.\u001b[39mwriteheader()\n\u001b[0;32m---> 39\u001b[0m writer\u001b[39m.\u001b[39mwriterows(row \u001b[39mfor\u001b[39;49;00m row \u001b[39min\u001b[39;49;00m reader)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/csv.py:111\u001b[0m, in \u001b[0;36mDictReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_num \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    109\u001b[0m     \u001b[39m# Used only for its side effect.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfieldnames\n\u001b[0;32m--> 111\u001b[0m row \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreader)\n\u001b[1;32m    112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_num \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreader\u001b[39m.\u001b[39mline_num\n\u001b[1;32m    114\u001b[0m \u001b[39m# unlike the basic reader, we prefer not to return blanks,\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39m# because we will typically wind up with a dict full of None\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m# values\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: I/O operation on closed file."
     ]
    }
   ],
   "source": [
    "# [10 pts] Fill in the missing \"symptoms\" values by the single most frequent symptoms in the province where the case was recorded. In case of a tie, use the symptoms that appears first in alphabetical order.\n",
    "\n",
    "def fill_missing_symptoms_values(f):\n",
    "    province_symptoms_counts = {}\n",
    "    \n",
    "    # Count occurrences of symptomss for each province\n",
    "    with open(f, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            if row['symptoms'].lower() != 'nan':\n",
    "                province = row['province']\n",
    "                symptoms = row['symptoms']\n",
    "                if province not in province_symptoms_counts:\n",
    "                    province_symptoms_counts[province] = Counter()\n",
    "                province_symptoms_counts[province][symptoms] += 1\n",
    "    \n",
    "    # Replace missing symptoms values\n",
    "    with open(f, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        header = reader.fieldnames\n",
    "        \n",
    "        for row in reader:\n",
    "            if row['symptoms'].lower() == 'nan':\n",
    "                province = row['province']\n",
    "                if province in province_symptoms_counts:\n",
    "                    most_common_symptomss = province_symptoms_counts[province].most_common()\n",
    "                    if most_common_symptomss:\n",
    "                        # Sort by symptoms name alphabetically in case of a tie\n",
    "                        most_common_symptomss.sort(key=lambda x: (x[1], x[0]))\n",
    "                        row['symptoms'] = most_common_symptomss[0][0]\n",
    "                else:\n",
    "                    # If no data for the province, fill with 'Unknown'\n",
    "                    row['symptoms'] = 'Unknown'\n",
    "\n",
    "    # Write the updated data back to the file\n",
    "    with open(f, 'w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(row for row in reader)\n",
    "\n",
    "# Call the function with the file path\n",
    "fill_missing_symptoms_values('../data/covidTrain.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[15 pts] Clean.\n",
    "Remove all characters that are not words or whitespaces. Words are sequences of letters (upper and lower case), digits, and underscores.\n",
    "Remove extra whitespaces between words. e.g., “Hello World! Let’s   learn    Python!”, so that there is exactly one whitespace between any pair of words.\n",
    "Remove all website links. A website link is a sequence of non-whitespace characters that starts with either \"http://\" or \"https://\".\n",
    "Convert all the words to lowercase.\n",
    "The resulting document should only contain lowercase words separated by a single whitespace.\n",
    "'''\n",
    "\n",
    "def clean(input_file, output_file):\n",
    "    cleaned_text = \"\"\n",
    "    with open(input_file, 'r') as file:\n",
    "        for line in file:\n",
    "            line = re.sub(r'http\\S+', '', line)\n",
    "            # Remove non-word characters except whitespaces\n",
    "            line = re.sub(r'[^\\w\\s]', '', line)\n",
    "            # Remove extra whitespaces\n",
    "            line = re.sub(r'\\s+', ' ', line)\n",
    "            line = line.lower()\n",
    "            cleaned_text += line\n",
    "            \n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(cleaned_text)\n",
    "\n",
    "clean('../data/test1.txt', '../hw2/test1_cleaned.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[7 pts] Remove stopwords.\n",
    "From the document that results after #1 above, remove \"stopwords\". These are the non-essential (or \"noise\") words listed in the file stopwords.txt\n",
    "'''\n",
    "\n",
    "def remove_stopwords(input_file, output_file, stopwords_file):\n",
    "    with open(stopwords_file, 'r') as stop_file:\n",
    "        stopwords = stop_file.read().splitlines()\n",
    "\n",
    "    cleaned_text = \"\"\n",
    "    with open(input_file, 'r') as file:\n",
    "        for line in file:\n",
    "            words = line.split()\n",
    "            cleaned_words = [word for word in words if word.lower() not in stopwords]\n",
    "            cleaned_line = ' '.join(cleaned_words)\n",
    "            cleaned_text += cleaned_line + '\\n'\n",
    "\n",
    "    # Write the cleaned text to the output file\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(cleaned_text)\n",
    "\n",
    "remove_stopwords('../hw2/test1_cleaned.txt', '../hw2/test1_remove_stopwords.txt', '../data/stopwords.txt')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[8 pts] Stemming and Lemmatization.\n",
    "This is a process of reducing words to their root forms. For example, look at the following reductions: run, running, runs → run. All three words capture the same idea ‘run’ and hence their suffixes are not as important.\n",
    "\n",
    "(If you would like to get a better idea, you may want read this article. This is completely optional, you can do the assignment without reading the article.)\n",
    "\n",
    "Use the following rules to reduce the words to their root form:\n",
    "\n",
    "Words ending with \"ing\": \"flying\" becomes \"fly\"\n",
    "Words ending with \"ly\": \"successfully\" becomes \"successful\"\n",
    "Words ending with \"ment\": \"punishment\" becomes \"punish\"\n",
    "'''\n",
    "\n",
    "def stemming_and_lemmatization(input_file, output_file):\n",
    "    pass\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
