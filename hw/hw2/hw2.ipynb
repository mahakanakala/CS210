{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "def calculate_fire_type_percentage(filename):\n",
    "    total_fire_pokemons = 0\n",
    "    fire_pokemons_above_level_40 = 0\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            if row['type'] == 'fire':\n",
    "                total_fire_pokemons += 1\n",
    "                if float(row['level']) >= 40:\n",
    "                    fire_pokemons_above_level_40 += 1\n",
    "    \n",
    "    percentage = (fire_pokemons_above_level_40 / total_fire_pokemons) * 100 if total_fire_pokemons > 0 else 0\n",
    "    return round(percentage)\n",
    "\n",
    "percentage = calculate_fire_type_percentage('../data/pokemonTrain.csv')\n",
    "\n",
    "with open('pokemon1.txt', 'w') as output_file:\n",
    "    output_file.write(f\"Percentage of fire type Pokemons at or above level 40 = {percentage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n",
    "[13 pts] Fill in the missing (NaN) values in the Attack (\"atk\"), Defense (\"def\") and Hit Points (\"hp\") columns as follows:\n",
    "- Set the pokemon level threshold to 40.\n",
    "- For a Pokemon having level above the threshold (i.e. > 40), fill in the missing value for atk/def/hp with the average values of atk/def/hp of Pokemons with level > 40. So, for instance, you would substitute the missing \"atk\" value for Magmar (level 44), with the average \"atk\" value for Pokemons with level > 40. Round the average to one decimal place.\n",
    "- For a Pokemon having level equal to or below the threshold (i.e. <= 40), fill in the missing value for atk/def/hp with the average values of atk/def/hp of Pokemons with level <= 40. Round the average to one decimal place.\n",
    "\n",
    "**After performing #2 and #3, write the modified data to another csv file named \"pokemonResult.csv\". This result file should have all of the rows from the input file - rows that were modified as well as rows that were not modified. If you do not write the modified data to another CSV file, or your output file name is not exactly as required, you will get 0 points.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "def get_most_common_type(weakness):\n",
    "    types = []\n",
    "    for w in weakness.split(','):\n",
    "        types.append(w.strip())\n",
    "    type_counts = Counter(types)\n",
    "    most_common_type = min(type_counts, key=lambda k: (-type_counts[k], k))\n",
    "    return most_common_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "def fill_missing_types(filename):\n",
    "    filled_data = []\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            if row['type'] == 'NaN':\n",
    "                most_common_type = get_most_common_type(row['weakness'])\n",
    "                row['type'] = most_common_type\n",
    "            filled_data.append(row)\n",
    "    return filled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(data):\n",
    "    atk_sum_over_40 = 0\n",
    "    def_sum_over_40 = 0\n",
    "    hp_sum_over_40 = 0\n",
    "    atk_count_over_40 = 0\n",
    "    def_count_over_40 = 0\n",
    "    hp_count_over_40 = 0\n",
    "    atk_sum_below_40 = 0\n",
    "    def_sum_below_40 = 0\n",
    "    hp_sum_below_40 = 0\n",
    "    atk_count_below_40 = 0\n",
    "    def_count_below_40 = 0\n",
    "    hp_count_below_40 = 0\n",
    "    \n",
    "    for row in data:\n",
    "        if row['atk'] != 'NaN':\n",
    "            if float(row['level']) > 40:\n",
    "                atk_sum_over_40 += float(row['atk'])\n",
    "                atk_count_over_40 += 1\n",
    "            else:\n",
    "                atk_sum_below_40 += float(row['atk'])\n",
    "                atk_count_below_40 += 1\n",
    "        \n",
    "        if row['def'] != 'NaN':\n",
    "            if float(row['level']) > 40:\n",
    "                def_sum_over_40 += float(row['def'])\n",
    "                def_count_over_40 += 1\n",
    "            else:\n",
    "                def_sum_below_40 += float(row['def'])\n",
    "                def_count_below_40 += 1\n",
    "        \n",
    "        if row['hp'] != 'NaN':\n",
    "            if float(row['level']) > 40:\n",
    "                hp_sum_over_40 += float(row['hp'])\n",
    "                hp_count_over_40 += 1\n",
    "            else:\n",
    "                hp_sum_below_40 += float(row['hp'])\n",
    "                hp_count_below_40 += 1\n",
    "    \n",
    "    avg_atk_over_40 = round(atk_sum_over_40 / atk_count_over_40, 1) if atk_count_over_40 > 0 else 0\n",
    "    avg_def_over_40 = round(def_sum_over_40 / def_count_over_40, 1) if def_count_over_40 > 0 else 0\n",
    "    avg_hp_over_40 = round(hp_sum_over_40 / hp_count_over_40, 1) if hp_count_over_40 > 0 else 0\n",
    "    avg_atk_below_40 = round(atk_sum_below_40 / atk_count_below_40, 1) if atk_count_below_40 > 0 else 0\n",
    "    avg_def_below_40 = round(def_sum_below_40 / def_count_below_40, 1) if def_count_below_40 > 0 else 0\n",
    "    avg_hp_below_40 = round(hp_sum_below_40 / hp_count_below_40, 1) if hp_count_below_40 > 0 else 0\n",
    "    \n",
    "    for row in data:\n",
    "        if row['atk'] == 'NaN':\n",
    "            if float(row['level']) > 40:\n",
    "                row['atk'] = str(avg_atk_over_40)\n",
    "            else:\n",
    "                row['atk'] = str(avg_atk_below_40)\n",
    "        \n",
    "        if row['def'] == 'NaN':\n",
    "            if float(row['level']) > 40:\n",
    "                row['def'] = str(avg_def_over_40)\n",
    "            else:\n",
    "                row['def'] = str(avg_def_below_40)\n",
    "        \n",
    "        if row['hp'] == 'NaN':\n",
    "            if float(row['level']) > 40:\n",
    "                row['hp'] = str(avg_hp_over_40)\n",
    "            else:\n",
    "                row['hp'] = str(avg_hp_below_40)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(data, filename):\n",
    "    with open(filename, 'w', newline='') as output_file:\n",
    "        fieldnames = data[0].keys()\n",
    "        writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "filename = '../data/pokemonTrain.csv'\n",
    "data = fill_missing_types(filename)\n",
    "modified_data = fill_missing_values(data)\n",
    "write_to_csv(modified_data, '../data/pokemonResult.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4\n",
    "[10 pts] Create a dictionary that maps pokemon types to their personalities. This dictionary would map a string to a list of strings. For example:\n",
    "     {\"fire\": [\"docile\", \"modest\", ...], \"normal\": [\"mild\", \"relaxed\", ...],  ...}\n",
    "Your dictionary should have the keys ordered alphabetically, and also items ordered alphabetically in the values list, as shown in the example above.\n",
    "\n",
    "Print the dictionary in the following format:\n",
    "\n",
    "   Pokemon type to personality mapping:\n",
    "\n",
    "      normal: mild, relaxed, ...\n",
    "      fire: docile, modest, ...\n",
    "      ...\n",
    "Print the dictionary to a file named \"pokemon4.txt\"\n",
    "If you do not print to a file, or your output file name is not exactly as required, you will get 0 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bug': {'careful'},\n",
       " 'electric': {'hardy'},\n",
       " 'fairy': {'impish', 'naughty', 'sassy'},\n",
       " 'fighting': {'adamant',\n",
       "  'calm',\n",
       "  'careful',\n",
       "  'lax',\n",
       "  'lonely',\n",
       "  'relaxed',\n",
       "  'serious'},\n",
       " 'fire': {'bold',\n",
       "  'brave',\n",
       "  'gentle',\n",
       "  'hardy',\n",
       "  'impish',\n",
       "  'lax',\n",
       "  'mild',\n",
       "  'modest',\n",
       "  'naive',\n",
       "  'naughty',\n",
       "  'quiet',\n",
       "  'rash',\n",
       "  'timid'},\n",
       " 'flying': {'hasty', 'impish', 'serious'},\n",
       " 'grass': {'adamant',\n",
       "  'bashful',\n",
       "  'bold',\n",
       "  'calm',\n",
       "  'docile',\n",
       "  'gentle',\n",
       "  'lonely',\n",
       "  'naive',\n",
       "  'quiet',\n",
       "  'sassy'},\n",
       " 'ground': {'bashful', 'gentle', 'hardy', 'quiet'},\n",
       " 'normal': {'brave', 'jolly', 'mild', 'naughty', 'quiet'},\n",
       " 'rock': {'bashful', 'docile', 'impish', 'mild', 'naughty'},\n",
       " 'water': {'bashful', 'bold', 'docile', 'hardy', 'impish', 'jolly', 'naughty'}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_type_to_personality_mapping(filename):\n",
    "    personality_mapping = {}\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            if row['type'] not in personality_mapping:\n",
    "                personality_mapping[row['type']] = set()\n",
    "            personality_mapping[row['type']].add(row['personality'])\n",
    "    \n",
    "    sorted_mapping = dict(sorted(personality_mapping.items()))\n",
    "    \n",
    "    with open('pokemon4.txt', 'w') as file:\n",
    "        file.write(\"Pokemon type to personality mapping:\\n\\n\")\n",
    "        for pokemon_type, personalities in sorted_mapping.items():\n",
    "            file.write(f\"{pokemon_type}: {', '.join(sorted(personalities))}\\n\")\n",
    "    \n",
    "    return sorted_mapping\n",
    "\n",
    "create_type_to_personality_mapping(filename = '../data/pokemonResult.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5\n",
    "[5 pts] Find out the average Hit Points (\"hp\") for pokemons of stage 3.0.\n",
    "Your program should print the value as follows (replace ... with value):\n",
    "\n",
    "Average hit point for Pokemons of stage 3.0 = ...\n",
    "You should round off the value, like in #1 above.\n",
    "\n",
    "Print the value to a file named \"pokemon5.txt\"\n",
    "If you do not print to a file, or your output file name is not exactly as required, you will get 0 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_average_hit_points(filename):\n",
    "    total_hit_points = 0\n",
    "    count_stage_3 = 0\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            if float(row['stage']) == 3.0:\n",
    "                count_stage_3 += 1\n",
    "                total_hit_points += float(row['hp'])\n",
    "    \n",
    "    average_hp = round(total_hit_points / count_stage_3) if count_stage_3 > 0 else 0\n",
    "    \n",
    "    with open('pokemon5.txt', 'w') as file:\n",
    "        file.write(f\"Average hit point for Pokemons of stage 3.0 = {average_hp}\")\n",
    "    \n",
    "    return average_hp\n",
    "calculate_average_hit_points('../data/pokemonResult.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Covid HW\n",
    "- [5 pts] In the age column, wherever there is a range of values, replace it by the rounded off average value. E.g., for 10-14 substitute 12. (Rounding should be done like in 1.1). You might want to use regular expressions here, but it is not required.\n",
    "- [6 pts] Change the date format for the date columns - date_onset_symptomssss, date_admission_hospital and date_confirmation from dd.mm.yyyy to mm.dd.yyyy. Again, you can use regexps here, but it is not required.\n",
    "- [7 pts] Fill in the missing (NaN) \"latitude\" and \"longitude\" values by the average of the latitude and longitude values for the province where the case was recorded. Round the average to 2 decimal places.\n",
    "- [7 pts] Fill in the missing “city” values by the most occurring city value in that province. In case of a tie, use the city that appears first in alphabetical order.\n",
    "- [10 pts] Fill in the missing \"symptoms\" values by the single most frequent symptoms in the province where the case was recorded. In case of a tie, use the symptoms that appears first in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [5 pts] In the age column, wherever there is a range of values, replace it by the rounded off average value. E.g., for 10-14 substitute 12. (Rounding should be done like in 1.1). You might want to use regular expressions here, but it is not required.\n",
    "\n",
    "def age_col(input_file):\n",
    "    edited_data = []\n",
    "    with open(input_file, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        header = next(reader)\n",
    "        edited_data.append(header) \n",
    "        for row in reader:\n",
    "            if '-' in row['age']:\n",
    "                ages = row['age'].split('-')\n",
    "                avg_age = (int(ages[0]) + int(ages[1])) / 2\n",
    "                row['age'] = str(avg_age)\n",
    "            edited_data.append(row)\n",
    "\n",
    "    return edited_data\n",
    "\n",
    "# age_col_edited_csv = age_col('../data/covidTrain.csv')\n",
    "# print(\"Edited CSV data:\")\n",
    "# for row in age_col_edited_csv:\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID,age,sex,city,province,country,latitude,longitude,date_onset_symptoms,date_admission_hospital,date_confirmation,symptoms\n",
      "11816,40-49,female,NaN,Aichi Prefecture,Japan,35.18333,136.9,19.02.2020,21.02.2020,22.02.2020,fever\n",
      "11814,60-69,male,Nagoya City,Aichi Prefecture,Japan,35.18333,136.9,02.19.2020,21.02.2020,22.02.2020,fever\n",
      "11813,60-69,male,Nagoya City,Aichi Prefecture,Japan,35.18333,136.9,02.19.2020,21.02.2020,22.02.2020,fever\n",
      "8504,60-69,male,NaN,Aichi Prefecture,Japan,35.2188941,136.9996761,02.17.2020,18.02.2020,18.02.2020,NaN\n",
      "8501,60-60,female,Nagoya City,Aichi Prefecture,Japan,35.18333,136.9,02.13.2020,14.02.2020,15.02.2020,fever; headache\n",
      "11815,60-69,female,Nagoya City,Aichi Prefecture,Japan,35.18333,136.9,02.19.2020,21.02.2020,22.02.2020,fever\n",
      "889,46,female,NaN,Anhui,China,32.21,117.02,01.19.2020,22.01.2020,26.01.2020,fever\n",
      "888,47,female,Ma'anshan City,Anhui,China,31.62885,118.35600000000001,01.22.2020,24.01.2020,26.01.2020,NaN\n",
      "887,55,male,NaN,Anhui,China,32.21,117.02,01.18.2020,24.01.2020,26.01.2020,fever\n",
      "886,47,male,Ma'anshan City,Anhui,China,32.21,117.02,01.22.2020,23.01.2020,25.01.2020,fever\n",
      "885,54,female,Ma'anshan City,Anhui,China,32.21,117.02,01.18.2020,22.01.2020,25.01.2020,fever\n",
      "884,48,female,Ma'anshan City,Anhui,China,32.21,117.02,01.19.2020,23.01.2020,25.01.2020,fever\n",
      "479,25,female,Bozhou City,Anhui,China,33.43671,116.1808,01.21.2020,23.01.2020,25.01.2020,fever; pharyngalgia\n",
      "478,35,male,Bozhou City,Anhui,China,33.43671,116.1808,01.19.2020,22.01.2020,25.01.2020,fever\n",
      "477,60,male,Bozhou City,Anhui,China,32.21,117.02,01.18.2020,22.01.2020,25.01.2020,NaN\n",
      "476,59,male,Bozhou City,Anhui,China,33.43671,116.1808,01.19.2020,21.01.2020,25.01.2020,cough; fever\n",
      "474,51,male,Bozhou City,Anhui,China,32.21,117.02,01.18.2020,21.01.2020,25.01.2020,NaN\n",
      "472,22,male,Fuyang City,Anhui,China,32.21,117.02,01.21.2020,23.01.2020,25.01.2020,fever; sore throat\n",
      "471,43,male,Fuyang City,Anhui,China,32.9188,115.7036,01.23.2020,23.01.2020,25.01.2020,fever\n",
      "466,25,female,NaN,Anhui,China,32.9188,115.7036,01.22.2020,22.01.2020,24.01.2020,fever\n",
      "883,43,male,NaN,Anhui,China,32.21,117.02,01.22.2020,22.01.2020,25.01.2020,fever\n",
      "1296,44,female,Bengbu City,Anhui,China,33.10901,117.32600000000001,01.24.2020,24.01.2020,27.01.2020,fever\n",
      "7,42,female,NaN,Anhui,China,32.21,117.02,01.21.2020,21.01.2020,22.01.2020,fever\n",
      "6749,36,male,Luyang District Hefei City,Anhui,China,32.21,117.02,01.26.2020,01.02.2020,05.02.2020,cough; fever\n",
      "4155,35,male,Hefei City,Anhui,China,31.79444,117.3428,01.30.2020,30.01.2020,31.01.2020,physical discomfort\n",
      "4154,32,male,Hefei City,Anhui,China,31.79444,117.3428,01.24.2020,28.01.2020,31.01.2020,NaN\n",
      "4153,27,female,NaN,Anhui,China,32.21,117.02,01.28.2020,29.01.2020,31.01.2020,cough; fever\n",
      "4152,45,female,Feidong County Hefei City,Anhui,China,32.00123,117.5681,01.25.2020,30.01.2020,31.01.2020,NaN\n",
      "4151,51,female,Lujiang County Hefei City,Anhui,China,31.269679999999997,117.32,01.24.2020,29.01.2020,31.01.2020,chills; headache; fever\n",
      "4150,37,male,Yaohai District Hefei City,Anhui,China,32.21,117.02,01.20.2020,23.01.2020,31.01.2020,fever; pharyngeal discomfort\n",
      "6739,55,female,NaN,Anhui,China,31.72762,117.0201,02.03.2020,05.02.2020,05.02.2020,chest tightness; fever\n",
      "1297,53,male,Bengbu City,Anhui,China,32.21,117.02,01.14.2020,19.01.2020,27.01.2020,fever\n",
      "6740,48,female,Yaohai District Hefei City,Anhui,China,31.89544,117.3384,01.30.2020,03.02.2020,05.02.2020,fever; joint pain; muscle soreness\n",
      "6742,35,male,Shushan District Hefei City,Anhui,China,32.21,117.02,01.29.2020,03.02.2020,05.02.2020,fever\n",
      "6743,29,male,Shushan District Hefei City,Anhui,China,31.82562,117.2001,02.01.2020,04.02.2020,05.02.2020,NaN\n",
      "6744,51,male,Lujiang County Hefei City,Anhui,China,31.269679999999997,117.32,02.02.2020,03.02.2020,05.02.2020,NaN\n",
      "6745,35,male,Luyang District Hefei City,Anhui,China,31.92046,117.1894,01.24.2020,03.02.2020,05.02.2020,chills; fever\n",
      "6746,28,female,NaN,Anhui,China,31.8947345,117.3260293,01.29.2020,02.02.2020,05.02.2020,cough; fever\n",
      "6747,78,female,Xinzhan District Hefei City,Anhui,China,32.21,117.02,01.22.2020,30.01.2020,05.02.2020,chest pain; nasal congestion\n",
      "6748,38,female,Xinzhan District Hefei City,Anhui,China,32.21,117.02,01.30.2020,03.02.2020,05.02.2020,cough; muscle soreness; sweating\n",
      "6741,43,female,Xinzhan District Hefei City,Anhui,China,31.8947345,117.3260293,02.03.2020,04.02.2020,05.02.2020,fever\n",
      "1511,28,male,Huainan City,Anhui,China,32.75738,116.734,01.22.2020,25.01.2020,26.01.2020,dry cough\n",
      "475,38,male,NaN,Anhui,China,32.21,117.02,01.20.2020,22.01.2020,25.01.2020,NaN\n",
      "15,45,male,Bengbu City,Anhui,China,33.10901,117.32600000000001,01.21.2020,21.01.2020,27.01.2020,fever\n",
      "14,38,female,Chizhou City,Anhui,China,30.28525,117.3658,01.22.2020,22.01.2020,23.01.2020,cough\n",
      "9,59,female,NaN,Anhui,China,32.21,117.02,01.19.2020,24.01.2020,26.01.2020,fever\n",
      "23,32,female,Haidian District,Beijing,China,39.9,116.35,01.17.2020,20.01.2020,21.01.2020,respiratory symptoms\n",
      "24,45,male,Shijingshan District,Beijing,China,39.92715,116.1737,01.19.2020,21.01.2020,22.01.2020,NaN\n",
      "25,45,male,Shijingshan District,Beijing,China,39.9,116.35,01.19.2020,21.01.2020,23.01.2020,fever\n",
      "26,18,female,Tongzhou District,Beijing,China,39.80292,116.7363,01.19.2020,20.01.2020,21.01.2020,fever; respiratory symptoms\n",
      "27,56,female,NaN,Beijing,China,39.91093,116.3591,01.16.2020,20.01.2020,21.01.2020,fever; respiratory symptoms\n",
      "28,42,male,NaN,Beijing,China,39.9,116.35,01.20.2020,20.01.2020,22.01.2020,fever\n",
      "22,39,male,Haidian District,Beijing,China,40.02186,116.2285,01.09.2020,14.01.2020,21.01.2020,fever\n",
      "21,37,male,NaN,Beijing,China,39.8306,116.2499,01.14.2020,20.01.2020,21.01.2020,fever\n",
      "89,43,male,Lanzhou City,Gansu,China,36.351259999999996,103.6554,01.18.2020,21.01.2020,23.01.2020,discomfort; fever\n",
      "90,24,male,Baiyin City,Gansu,China,36.61864,104.6208,01.16.2020,22.01.2020,23.01.2020,cough; fever; headache; muscular soreness; weak\n",
      "8904,42,female,Longxi County Dingxi City,Gansu,China,35.71,104.59,01.28.2020,04.02.2020,05.02.2020,NaN\n",
      "8905,26,female,Longxi County Dingxi City,Gansu,China,35.10665,104.6236,02.01.2020,04.02.2020,05.02.2020,fever; cough\n",
      "8906,37,male,Baiyin District Baiyin City,Gansu,China,35.71,104.59,01.24.2020,29.01.2020,05.02.2020,NaN\n",
      "8907,24,male,Ningxian County Qingyang City,Gansu,China,35.71,104.59,02.01.2020,01.02.2020,05.02.2020,fever; runny nose\n",
      "8915,29,female,Longxi County Dingxi City,Gansu,China,35.71,104.59,01.24.2020,04.02.2020,07.02.2020,headache; dry mouth\n",
      "8914,54,female,Longxi County Dingxi City,Gansu,China,35.10665,104.6236,01.21.2020,04.02.2020,07.02.2020,dry throat; sore throat; fatigue\n",
      "4280,31,male,Chengguan District Lanzhou City,Gansu,China,35.71,104.59,01.19.2020,30.01.2020,01.02.2020,discomfort\n",
      "4281,66,female,Chengguan District Lanzhou City,Gansu,China,35.71,104.59,01.24.2020,28.01.2020,01.02.2020,NaN\n",
      "4283,32,male,NaN,Gansu,China,35.71,104.59,01.24.2020,28.01.2020,01.02.2020,fever\n",
      "4284,60,male,Gannan Prefecture,Gansu,China,34.33256,102.7657,01.28.2020,30.01.2020,01.02.2020,fever; cough\n",
      "5120,26,male,Chengguan District Lanzhou City,Gansu,China,36.07809,103.8756,01.26.2020,30.01.2020,02.02.2020,NaN\n",
      "5121,73,female,Xigu District Lanzhou City,Gansu,China,36.12968,103.51899999999999,01.28.2020,31.01.2020,02.02.2020,NaN\n",
      "5122,44,male,Xigu District Lanzhou City,Gansu,China,36.12968,103.51899999999999,01.23.2020,26.01.2020,02.02.2020,cough; fever; sputum\n",
      "5123,36,male,Ningxian County Qingyang City,Gansu,China,35.565909999999995,108.1031,01.24.2020,28.01.2020,02.02.2020,cough; fever\n",
      "5124,38,female,Linxia City Linxia Prefecture,Gansu,China,35.58106,103.1937,01.27.2020,01.02.2020,02.02.2020,cough; fever\n",
      "5126,64,female,Maiji District Tianshui City,Gansu,China,35.71,104.59,01.23.2020,30.01.2020,02.02.2020,cough; sputum\n",
      "5127,59,male,Maiji District Tianshui City,Gansu,China,35.71,104.59,01.24.2020,30.01.2020,02.02.2020,NaN\n",
      "5128,39,female,Maiji District Tianshui City,Gansu,China,35.71,104.59,01.30.2020,30.01.2020,02.02.2020,fever\n",
      "5129,46,female,Qinzhou District Tianshui City,Gansu,China,35.71,104.59,01.28.2020,30.01.2020,02.02.2020,chills; fatigue\n",
      "8916,38,female,Longxi County Dingxi City,Gansu,China,35.71,104.59,01.21.2020,04.02.2020,07.02.2020,fever; dry mouth; throat discomfort\n",
      "8922,55,female,Maiji District Tianshui City,Gansu,China,34.415,106.1453,01.29.2020,31.01.2020,08.02.2020,pharyngeal discomfort\n",
      "6442,69,male,Chengguan District Lanzhou City,Gansu,China,35.71,104.59,01.28.2020,28.01.2020,04.02.2020,fatigue\n",
      "8927,33,female,Huating County Pingliang City,Gansu,China,35.71,104.59,01.29.2020,07.02.2020,09.02.2020,NaN\n",
      "8933,53,female,Gannan Prefecture,Gansu,China,35.71,104.59,02.04.2020,11.02.2020,12.02.2020,dry throat; dry cough\n",
      "8934,34,female,Huating County Pingliang City,Gansu,China,35.71,104.59,02.10.2020,11.02.2020,13.02.2020,NaN\n",
      "8937,57,male,Chengguan District Lanzhou City,Gansu,China,36.07809,103.8756,02.07.2020,14.02.2020,16.02.2020,cough; fever\n",
      "8929,27,female,NaN,Gansu,China,36.04074,107.6714,02.03.2020,06.02.2020,09.02.2020,fever\n",
      "8925,46,female,NaN,Gansu,China,36.12968,103.51899999999999,02.05.2020,07.02.2020,08.02.2020,NaN\n",
      "8924,1,female,Huating County Pingliang City,Gansu,China,35.20198,106.5993,01.30.2020,07.02.2020,08.02.2020,diarrhea\n",
      "8923,27,female,Huating County Pingliang City,Gansu,China,35.71,104.59,02.01.2020,04.02.2020,08.02.2020,fever\n",
      "8928,23,male,Zhuanglang County Pingliang City,Gansu,China,35.71,104.59,02.06.2020,08.02.2020,09.02.2020,cough; sputum; fever\n",
      "12037,52,female,Qilihe District Lanzhou City,Gansu,China,35.71,104.59,01.25.2020,27.01.2020,30.01.2020,dry cough\n",
      "12034,57,male,NaN,Gansu,China,36.07809,103.8756,02.07.2020,14.02.2020,16.02.2020,NaN\n",
      "12038,41,male,Chengguan District Lanzhou City,Gansu,China,36.07809,103.8756,01.16.2020,27.01.2020,30.01.2020,cough; sore body; cold\n",
      "100,65,female,Shenzhen City,Guangdong,China,22.65389,114.1291,01.03.2020,10.01.2020,21.01.2020,cough; fever; weakness\n",
      "122,66,female,NaN,Guangdong,China,22.16925,113.361,01.13.2020,19.01.2020,21.01.2020,cough\n",
      "104,37,female,Shenzhen City,Guangdong,China,22.53,113.94,01.02.2020,11.01.2020,21.01.2020,diarrhea; fever; nasal congestion; pleuritic chest pain; sore throat\n",
      "103,63,female,Shenzhen City,Guangdong,China,22.53,113.94,01.08.2020,15.01.2020,21.01.2020,anhelation; cough; fever; pleural effusion; weakness\n",
      "101,36,male,Shenzhen City,Guangdong,China,22.65389,114.1291,01.01.2020,11.01.2020,21.01.2020,cough; diarrhea; fever; rhinorrhoea; sneezing\n",
      "99,66,male,NaN,Guangdong,China,22.65389,114.1291,01.04.2020,10.01.2020,19.01.2020,NaN\n",
      "91,40,male,Foshan City,Guangdong,China,22.53,113.94,01.10.2020,20.01.2020,22.01.2020,cough; dizziness\n",
      "5199,47,male,NaN,Guangxi,China,21.88401,107.9967,01.26.2020,31.01.2020,02.02.2020,fever; dry cough; fatigue\n",
      "6512,62,male,NaN,Guangxi,China,21.88401,107.9967,01.29.2020,01.02.2020,04.02.2020,fever; fatigue\n",
      "11927,52,female,Fangchenggang City,Guangxi,China,21.88401,107.9967,01.16.2020,29.01.2020,03.02.2020,chest tightness; fatigue\n",
      "9152,60,male,Fangchenggang City,Guangxi,China,21.88401,107.9967,01.28.2020,04.02.2020,07.02.2020,NaN\n",
      "6513,60,female,Fangchenggang City,Guangxi,China,21.88401,107.9967,02.02.2020,03.02.2020,05.02.2020,fever; sore throat;fatigue;vomiting\n",
      "9181,38,female,NaN,Guangxi,China,21.88401,107.9967,01.28.2020,28.01.2020,30.01.2020,fever; fatigue\n",
      "6514,45,female,Fangchenggang City,Guangxi,China,22.63,108.56,01.30.2020,03.02.2020,05.02.2020,cough\n",
      "11926,62,male,Fangchenggang City,Guangxi,China,21.88401,107.9967,01.25.2020,29.01.2020,03.02.2020,fever; fatigue\n",
      "11911,10,male,Fangchenggang City,Guangxi,China,22.63,108.56,02.07.2020,08.02.2020,09.02.2020,fever\n",
      "9180,65,male,Fangchenggang City,Guangxi,China,22.63,108.56,01.13.2020,28.01.2020,30.01.2020,NaN\n",
      "150,34,male,Guilin City,Guangxi,China,25.35865,110.5145,01.19.2020,22.01.2020,24.01.2020,cough; fever\n",
      "6511,54,female,Fangchenggang City,Guangxi,China,22.63,108.56,01.30.2020,01.02.2020,04.02.2020,fever;sore throat; fatigue; diarrhea\n",
      "166,61,male,Yulin City,Guangxi,China,22.44296,110.182,01.21.2020,22.01.2020,24.01.2020,NaN\n",
      "148,46,male,NaN,Guangxi,China,22.63,108.56,01.20.2020,21.01.2020,22.01.2020,fever\n",
      "147,49,male,Liuzhou City,Guangxi,China,22.63,108.56,01.21.2020,21.01.2020,22.01.2020,fever\n",
      "956,47,female,Fangchenggang City,Guangxi,China,22.63,108.56,01.23.2020,25.01.2020,26.01.2020,fever; fatigue; dry cough\n",
      "152,72,female,Beihai City,Guangxi,China,21.676660000000002,109.32799999999999,01.19.2020,19.01.2020,24.01.2020,fever; weak\n",
      "153,20,female,Beihai City,Guangxi,China,22.63,108.56,01.22.2020,22.01.2020,24.01.2020,NaN\n",
      "154,54,male,NaN,Guangxi,China,21.676660000000002,109.32799999999999,01.22.2020,22.01.2020,24.01.2020,cough; fever\n",
      "146,62,female,Beihai City,Guangxi,China,22.63,108.56,01.09.2020,21.01.2020,22.01.2020,cough; fever\n",
      "145,63,male,NaN,Guangxi,China,22.63,108.56,01.14.2020,21.01.2020,22.01.2020,NaN\n",
      "11970,44,female,Fangchenggang City,Guangxi,China,21.88401,107.9967,01.22.2020,27.01.2020,28.01.2020,cough; fever\n",
      "155,33,female,NaN,Guangxi,China,22.63,108.56,01.19.2020,21.01.2020,24.01.2020,fever\n",
      "156,49,female,Hechi City,Guangxi,China,24.64574,107.8409,01.21.2020,22.01.2020,24.01.2020,chest distress; cough; expectoration; muscular soreness\n",
      "162,33,male,Hechi City,Guangxi,China,24.64574,107.8409,01.19.2020,21.01.2020,24.01.2020,cough; headache\n",
      "163,2,female,Hechi City,Guangxi,China,22.63,108.56,01.22.2020,23.01.2020,24.01.2020,fever; sneeze\n",
      "151,41,male,Guilin City,Guangxi,China,25.35865,110.5145,01.20.2020,23.01.2020,24.01.2020,fever\n",
      "164,29,male,NaN,Guangxi,China,21.88401,107.9967,01.23.2020,23.01.2020,24.01.2020,discomfort\n",
      "165,62,male,NaN,Guangxi,China,21.88401,107.9967,01.15.2020,23.01.2020,24.01.2020,NaN\n",
      "11971,55,female,NaN,Guangxi,China,22.63,108.56,01.22.2020,26.01.2020,28.01.2020,fever; nasal congestion; runny nose; sore throat; cough\n",
      "167,20,male,NaN,Guizhou,China,26.43055,107.1928,01.18.2020,18.01.2020,22.01.2020,discomfort\n",
      "169,50,female,Tongren City,Guizhou,China,27.3,107.14,01.14.2020,21.01.2020,22.01.2020,NaN\n",
      "8489,68,male,Zunyi City,Guizhou,China,28.171429999999997,107.0848,01.31.2020,03.02.2020,07.02.2020,nausea; vomiting\n",
      "11550,0-10,male,NaN,Hokkaido,Japan,43.4115473,142.3833135,02.15.2020,19.02.2020,21.02.2020,fever 37.7 C\n",
      "11551,0-10,male,Nakafurano,Hokkaido,Japan,43.4115473,142.3833135,02.18.2020,19.02.2020,21.02.2020,fever 37.7 C\n",
      "12576,30-39,male,Okhotsk,Hokkaido,Japan,43.09,142.37,02.17.2020,24.02.2020,27.02.2020,sore throat; fever (38-39 C)\n",
      "12554,70-79,female,Shinhidaka,Hokkaido,Japan,42.4384206,142.3325668,02.17.2020,24.02.2020,26.02.2020,fever (38-39 C); malaise\n",
      "4109,39,male,Hong Kong,Hong Kong,China,22.38074,114.1324,01.19.2020,30.01.2020,01.02.2020,cough; fever\n",
      "1085,64,male,NaN,Hong Kong,China,22.38074,114.1324,01.24.2020,25.01.2020,26.01.2020,fever\n",
      "9670,51,male,Tsing Yi,Hong Kong,China,22.35,114.15,02.03.2020,10.02.2020,12.02.2020,fever\n",
      "9672,67,female,Quarry Bay,Hong Kong,China,22.2859106,114.20686780000001,01.31.2020,12.02.2020,13.02.2020,chills; cough; fever\n",
      "9673,37,male,Wan Chai,Hong Kong,China,22.35,114.15,02.08.2020,12.02.2020,13.02.2020,fever\n",
      "7670,60,male,Lam Tin,Hong Kong,China,22.35,114.15,01.22.2020,30.01.2020,04.02.2020,fever; myalgia; shortness of breath\n",
      "11865,96,female,NaN,Hong Kong,China,22.2826304,114.18812109999999,02.13.2020,22.02.2020,22.02.2020,fever; cough\n",
      "3052,72,male,NaN,Hong Kong,China,22.38074,114.1324,01.25.2020,29.01.2020,28.01.2020,fever\n",
      "1084,68,female,Hong Kong,Hong Kong,China,22.35,114.15,01.21.2020,25.01.2020,26.01.2020,cough; fever\n",
      "3681,37,female,Hong Kong,Hong Kong,China,22.35,114.15,01.28.2020,30.01.2020,31.01.2020,cough\n",
      "3682,75,male,Hong Kong,Hong Kong,China,22.38074,114.1324,01.22.2020,24.01.2020,31.01.2020,cough; dyspnea\n",
      "4903,80,male,New Territories,Hong Kong,China,22.406999999999996,114.12200000000001,01.19.2020,30.01.2020,01.02.2020,cough; fever\n",
      "5161,72,female,Block 1 Site 11 Whampao Garden,Hong Kong,China,22.304412699999997,114.1870613,02.01.2020,01.02.2020,02.02.2020,cough\n",
      "3053,73,female,Hong Kong,Hong Kong,China,22.38074,114.1324,01.25.2020,29.01.2020,28.01.2020,fever\n",
      "12545,60,Female,NaN,Hong Kong,China,22.38074,114.1324,02.12.2020,24.02.2020,24.02.2020,cough\n",
      "6353,64,female,Kowloon,Hong Kong,China,22.319751,114.18611999999999,01.23.2020,01.02.2020,04.02.2020,NaN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# [6 pts] Change the date format for the date columns - date_onset_symptomss, date_admission_hospital and date_confirmation from dd.mm.yyyy to mm.dd.yyyy. Again, you can use regexps here, but it is not required.\n",
    "\n",
    "# def date_col(f):\n",
    "#     edited_data = []\n",
    "#     with open(f, 'r') as file:\n",
    "#         reader = csv.DictReader(file)\n",
    "#         header = next(reader)\n",
    "#         edited_data.append(header)\n",
    "#         for row in reader:\n",
    "#             if '.' in row['date_onset_symptomss']:\n",
    "#                 date = row['date_onset_symptomss'].split('.')\n",
    "#                 day, month, year = date[0], date[1], date[2]\n",
    "#                 row['date_onset_symptomss'] = month + \".\" + day + \".\"+ year\n",
    "#                 # print(row['date_onset_symptomss'])\n",
    "#             edited_data.append(row)\n",
    "#     return \n",
    "\n",
    "# age_col_edited_csv = age_col('../data/covidTrain.csv')\n",
    "# print(\"Edited CSV data:\")\n",
    "# for row in age_col_edited_csv:\n",
    "#     print(row)\n",
    "\n",
    "import csv\n",
    "\n",
    "def date_col(f):\n",
    "    edited_data = []\n",
    "    with open(f, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        header = next(reader)\n",
    "        edited_data.append(header)\n",
    "        for row in reader:\n",
    "            if '.' in row['date_onset_symptoms']:\n",
    "                date = row['date_onset_symptoms'].split('.')\n",
    "                day, month, year = date[0], date[1], date[2]\n",
    "                row['date_onset_symptoms'] = month + \".\" + day + \".\"+ year\n",
    "            edited_data.append(row)\n",
    "    \n",
    "    # Convert the list of dictionaries to a CSV-formatted string\n",
    "    csv_string = ','.join(header) + '\\n'\n",
    "    for row in edited_data:\n",
    "        csv_string += ','.join(row.values()) + '\\n'\n",
    "    \n",
    "    return csv_string\n",
    "\n",
    "# Example usage\n",
    "edited_csv_string = date_col('../data/covidTrain.csv')\n",
    "print(edited_csv_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [7 pts] Fill in the missing (NaN) \"latitude\" and \"longitude\" values by the average of the latitude and longitude values for the province where the case was recorded. Round the average to 2 decimal places.\n",
    "\n",
    "def replace_empty_vals(input_file):\n",
    "    edited_data = []\n",
    "    province_data = defaultdict(list)\n",
    "\n",
    "    # Read the input CSV file and store data by province\n",
    "    with open(input_file, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        header = next(reader)\n",
    "        edited_data.append(header)  # Include header in the edited data\n",
    "        for row in reader:\n",
    "            province_data[row['province']].append(row)\n",
    "\n",
    "    # Calculate the average latitude and longitude for each province\n",
    "    for province, data in province_data.items():\n",
    "        lat_sum = 0\n",
    "        long_sum = 0\n",
    "        count = 0\n",
    "        for entry in data:\n",
    "            if entry['latitude'].lower() != 'nan' and entry['longitude'].lower() != 'nan':\n",
    "                lat_sum += float(entry['latitude'])\n",
    "                long_sum += float(entry['longitude'])\n",
    "                count += 1\n",
    "\n",
    "        if count > 0:\n",
    "            lat_avg = round(lat_sum / count, 2)\n",
    "            long_avg = round(long_sum / count, 2)\n",
    "\n",
    "            # Update missing latitude and longitude values with the average\n",
    "            for entry in data:\n",
    "                if entry['latitude'].lower() == 'nan' or entry['longitude'].lower() == 'nan':\n",
    "                    entry['latitude'] = str(lat_avg)\n",
    "                    entry['longitude'] = str(long_avg)\n",
    "\n",
    "            edited_data.extend(data)  # Append the edited data to the list\n",
    "\n",
    "    return edited_data\n",
    "\n",
    "# edited_csv_data = replace_empty_vals('../data/covidTrain.csv')\n",
    "# for row in edited_csv_data:\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [7 pts] Fill in the missing “city” values by the most occurring city value in that province. In case of a tie, use the city that appears first in alphabetical order.\n",
    "\n",
    "def fill_missing_city_values(file_path):\n",
    "    # Count occurrences of cities for each province\n",
    "    province_city_counts = defaultdict(Counter)\n",
    "    updated_rows = []\n",
    "    \n",
    "    # Read the file to count city occurrences\n",
    "    with open(file_path, 'r', newline='') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            city = row['city'].strip().lower()\n",
    "            province = row['province'].strip().lower()\n",
    "            if city != 'nan':\n",
    "                province_city_counts[province][city] += 1\n",
    "    \n",
    "    # Replace missing city values\n",
    "    with open(file_path, 'r', newline='') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            city = row['city'].strip().lower()\n",
    "            province = row['province'].strip().lower()\n",
    "            if city == 'nan':\n",
    "                if province in province_city_counts:\n",
    "                    most_common_cities = province_city_counts[province].most_common()\n",
    "                    if most_common_cities:\n",
    "                        # Sort by city name alphabetically in case of a tie\n",
    "                        most_common_cities.sort(key=lambda x: (x[1], x[0]))\n",
    "                        row['city'] = most_common_cities[0][0]\n",
    "                else:\n",
    "                    # If no data for the province, fill with 'Unknown'\n",
    "                    row['city'] = 'Unknown'\n",
    "            updated_rows.append(row)\n",
    "            # print(updated_rows)\n",
    "    \n",
    "    return updated_rows\n",
    "\n",
    "# Call the function with the file path\n",
    "updated_rows = fill_missing_city_values('../data/covidTrain.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_symptoms_values(file_path):\n",
    "    province_symptoms_counts = {}\n",
    "\n",
    "    # Count occurrences of symptoms for each province\n",
    "    with open(file_path, 'r', newline='') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            symptoms = row['symptoms'].strip().lower()\n",
    "            province = row['province'].strip().lower()\n",
    "            if symptoms != 'nan':\n",
    "                if province not in province_symptoms_counts:\n",
    "                    province_symptoms_counts[province] = Counter()\n",
    "                province_symptoms_counts[province][symptoms] += 1\n",
    "\n",
    "    # Replace missing symptoms values\n",
    "    updated_rows = []\n",
    "    with open(file_path, 'r', newline='') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        header = reader.fieldnames\n",
    "        for row in reader:\n",
    "            symptoms = row['symptoms'].strip().lower()\n",
    "            province = row['province'].strip().lower()\n",
    "            if symptoms == 'nan':\n",
    "                if province in province_symptoms_counts:\n",
    "                    most_common_symptoms = province_symptoms_counts[province].most_common()\n",
    "                    if most_common_symptoms:\n",
    "                        # Sort by symptoms name alphabetically in case of a tie\n",
    "                        most_common_symptoms.sort(key=lambda x: (x[1], x[0]))\n",
    "                        row['symptoms'] = most_common_symptoms[0][0]\n",
    "                else:\n",
    "                    # If no data for the province, fill with 'Unknown'\n",
    "                    row['symptoms'] = 'Unknown'\n",
    "            updated_rows.append(row)\n",
    "\n",
    "    return updated_rows\n",
    "\n",
    "# updated_csv_data = fill_missing_symptoms_values('../data/covidTrain.csv')\n",
    "# for row in updated_csv_data:\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m         writer\u001b[39m.\u001b[39mwriterows(csv_data)\n\u001b[1;32m     22\u001b[0m \u001b[39m# Example usage\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m covid_file_name(\u001b[39m'\u001b[39;49m\u001b[39m../data/covidTrain.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m, in \u001b[0;36mcovid_file_name\u001b[0;34m(input_file, output_file)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Call each function sequentially\u001b[39;00m\n\u001b[1;32m      6\u001b[0m edited_csv \u001b[39m=\u001b[39m age_col(input_file)\n\u001b[0;32m----> 7\u001b[0m edited_csv \u001b[39m=\u001b[39m date_col(edited_csv)\n\u001b[1;32m      8\u001b[0m edited_csv \u001b[39m=\u001b[39m replace_empty_vals(edited_csv)\n\u001b[1;32m      9\u001b[0m edited_csv \u001b[39m=\u001b[39m fill_missing_symptoms_values(edited_csv)\n",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36mdate_col\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdate_col\u001b[39m(f):\n\u001b[1;32m      4\u001b[0m     edited_data \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(f, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      6\u001b[0m         reader \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mDictReader(file)\n\u001b[1;32m      7\u001b[0m         header \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(reader)\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py:279\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(io_open)\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_modified_open\u001b[39m(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;49;00m {\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m}:\n\u001b[1;32m    280\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m         )\n\u001b[1;32m    286\u001b[0m     \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "def covid_file_name(input_file, output_file=None):\n",
    "    if output_file is None:\n",
    "        output_file = f'covidResult.csv'\n",
    "        \n",
    "    age_col_edited_csv = age_col(input_file)\n",
    "    date_col_edited_csv = date_col(age_col_edited_csv)\n",
    "    replace_empty_vals_csv = replace_empty_vals(date_col_edited_csv)\n",
    "    fill_missing_symptoms_values_csv = fill_missing_symptoms_values(replace_empty_vals_csv)\n",
    "    \n",
    "    with open(output_file, 'w', newline='') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fill_missing_symptoms_values_csv[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(fill_missing_symptoms_values_csv)\n",
    "\n",
    "covid_file_name('../data/covidTrain.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[15 pts] Clean.\n",
    "Remove all characters that are not words or whitespaces. Words are sequences of letters (upper and lower case), digits, and underscores.\n",
    "Remove extra whitespaces between words. e.g., “Hello World! Let’s   learn    Python!”, so that there is exactly one whitespace between any pair of words.\n",
    "Remove all website links. A website link is a sequence of non-whitespace characters that starts with either \"http://\" or \"https://\".\n",
    "Convert all the words to lowercase.\n",
    "The resulting document should only contain lowercase words separated by a single whitespace.\n",
    "'''\n",
    "\n",
    "def clean(text):\n",
    "    cleaned_text = \"\"\n",
    "    cleaned_text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    cleaned_text = re.sub(r'\\[.*?\\]', '', cleaned_text)  # Remove square brackets and content inside them\n",
    "    cleaned_text = re.sub(r'\\(.*?\\)', '', cleaned_text)  # Remove parentheses and content inside them\n",
    "    cleaned_text = re.sub(r'\\{.*?\\}', '', cleaned_text)  # Remove curly brackets and content inside them\n",
    "    cleaned_text = re.sub(r'\\n', ' ', cleaned_text)  # Replace newline characters with space\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Replace multiple consecutive spaces with a single space\n",
    "    cleaned_text = cleaned_text.strip().lower()  # Remove leading and trailing spaces\n",
    "    \n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[7 pts] Remove stopwords.\n",
    "From the document that results after #1 above, remove \"stopwords\". These are the non-essential (or \"noise\") words listed in the file stopwords.txt\n",
    "'''\n",
    "\n",
    "def remove_stopwords(text, stopwords_file):\n",
    "    with open(stopwords_file, 'r') as stop_file:\n",
    "        stopwords = stop_file.read().splitlines()\n",
    "\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords]\n",
    "    return ' '.join(filtered_words)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[8 pts] Stemming and Lemmatization.\n",
    "This is a process of reducing words to their root forms. For example, look at the following reductions: run, running, runs → run. All three words capture the same idea ‘run’ and hence their suffixes are not as important.\n",
    "\n",
    "(If you would like to get a better idea, you may want read this article. This is completely optional, you can do the assignment without reading the article.)\n",
    "\n",
    "Use the following rules to reduce the words to their root form:\n",
    "\n",
    "Words ending with \"ing\": \"flying\" becomes \"fly\"\n",
    "Words ending with \"ly\": \"successfully\" becomes \"successful\"\n",
    "Words ending with \"ment\": \"punishment\" becomes \"punish\"\n",
    "'''\n",
    "\n",
    "def stemming_and_lemmatization(text):\n",
    "        stemming_and_lemmatization = \"\"\n",
    "        \n",
    "        for word in text.split():\n",
    "            if word.endswith('ing'):\n",
    "                word = re.sub(r'ing$', '', word)\n",
    "            elif word.endswith('ly'):\n",
    "                word = re.sub(r'ly$', '', word)\n",
    "            elif word.endswith('ment'):\n",
    "                word = re.sub(r'ment$', '', word)\n",
    "            elif word.endswith('ed'):\n",
    "                word = re.sub(r'ed$', '', word)\n",
    "            stemming_and_lemmatization += word + \" \"\n",
    "            \n",
    "        return stemming_and_lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(input_file, output_file=None):\n",
    "    if output_file is None:\n",
    "        file_name = input_file.split(\"/\")[-1].split(\".\")[0]\n",
    "        output_file = f'preproc_{file_name}.txt'\n",
    "        \n",
    "    with open(input_file, 'r') as infile:\n",
    "        text = infile.read()\n",
    "        cleaned_text = clean(text)\n",
    "        text_no_stopwords = remove_stopwords(cleaned_text, '../data/stopwords.txt')\n",
    "        preprocessed_text = stemming_and_lemmatization(text_no_stopwords)\n",
    "    \n",
    "    with open(output_file, 'w') as outfile:\n",
    "        outfile.write(preprocessed_text)\n",
    "\n",
    "preprocess_text('../data/test1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Part 2: Computing TF-IDF Scores (30 pts)\n",
    "Once preprocessing is performed on all the documents, you need to compute the Term Frequency(TF) — Inverse Document Frequency(IDF) score for each word.\n",
    "\n",
    "What is TF-IDF?\n",
    "\n",
    "In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "\n",
    "Resources:\n",
    "\n",
    "TFIDF Python Example\n",
    "tf-idf Wikipedia Page\n",
    "TF-IDF/Term Frequency Technique\n",
    "Steps:\n",
    "\n",
    "- For each preprocessed document that results from the preprocessing in Part 1, compute frequencies of all the distinct words in that document only. So if you had 3 documents in the input set, you will compute 3 sets of word frequencies, one per document.\n",
    "- Compute the Term Frequency (TF) of each distinct word (also called term) for each of the preprocessed documents:\n",
    "    TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "Note: The denominator, total number of terms, is the sum total of all the words, not just unique instances. So if a word occurs 5 times, and the total number of words in a document is 100, then TF for that word is 5/100.\n",
    "\n",
    "- Compute the Inverse Document Frequency (IDF) of each distinct word for each of the preprocessed documents.\n",
    "IDF is a measure of how common or rare a word is in a document set (a set of preprocessd text files in this case). It is calculated by taking the logarithm of the following term:\n",
    "\n",
    "   IDF(t) = log((Total number of documents) / (Number of documents the word is found in)) + 1\n",
    "Note: The log here uses base e. And 1 is added after the log is taken, so that the IDF score is guaranteed to be non-zero.\n",
    "- Calculate TF-IDF score: TF * IDF for each distinct word in each preprocessed document. Round the score to 2 decimal places.\n",
    "- Print the top 5 most important words in each preprocessed document according to their TF-IDF scores. The higher the TF-IDF score, the more important the word. In case of ties in score, pick words in alphabetical order. You should print the result as a list of (word,TF-IDF score) tuples sorted in descending TF-IDF scores. See the Testing section below, in files tfidf_test1.txt and tfidf_test2.txt, for the exact output format.\n",
    "\n",
    "\n",
    "Print to a file prefixed with \"tfidf_\". So if the initial input document was \"doc1.txt\", you should print the TF-IDF results to \"tfidf_doc1.txt\".\n",
    "If you do not print to a file, or your output file name is not exactly as required, you will get 0 points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_tf_idf(preprocessed_documents):\n",
    "    tf_idf_results = {}\n",
    "    for doc_name, preprocessed_doc in preprocessed_documents.items():\n",
    "        # Step 1: Compute TF\n",
    "        term_frequency = {}\n",
    "        total_terms = len(preprocessed_doc)\n",
    "        for term in preprocessed_doc:\n",
    "            term_frequency[term] = term_frequency.get(term, 0) + 1\n",
    "        \n",
    "        # Step 2: Compute IDF\n",
    "        document_frequency = {}\n",
    "        for term in set(preprocessed_doc):\n",
    "            for _, other_doc in preprocessed_documents.items():\n",
    "                if term in other_doc:\n",
    "                    document_frequency[term] = document_frequency.get(term, 0) + 1\n",
    "        num_documents = len(preprocessed_documents)\n",
    "        inverse_document_frequency = {term: math.log(num_documents / (document_frequency[term] + 1)) + 1 for term in document_frequency}\n",
    "        \n",
    "        # Step 3: Calculate TF-IDF\n",
    "        tf_idf_scores = {term: (term_frequency[term] / total_terms) * inverse_document_frequency[term] for term in term_frequency}\n",
    "        \n",
    "        # Step 4: Sort and store TF-IDF scores\n",
    "        sorted_tf_idf = sorted(tf_idf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_5 = sorted_tf_idf[:5]\n",
    "        tf_idf_results[doc_name] = top_5\n",
    "        \n",
    "        # Step 5: Write results to file\n",
    "        output_file = f'tfidf_{doc_name}'\n",
    "        with open(output_file, 'w') as outfile:\n",
    "            for word, score in top_5:\n",
    "                outfile.write(f'{word}: {score:.2f}\\n')\n",
    "    \n",
    "    return tf_idf_results\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
